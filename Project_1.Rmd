---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 12"
author: "Emma Skarnes, HÃ¥kon Noren  and Alexander Johan Arntzen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)

```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```


# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex.

## a)
The expected test mean squared error (MSE) at "x_{0}" is 
$$
 E[y_{0} -\hat{f}(x_{0}) ]^{2}.
 \eqref{eq:1}
$$

Where $y_{0}$ is the new observation.
## b)

Firstly we from computation it is clear that 
$$
\begin{align}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathrm{E}}
\E[ f( x_{0}) -\hat{f}( x_{0})]^{2} & =\E[ f( x_{0}) -\E[\hat{f}( x_{0})] +\E[\hat{f}( x_{0})] -\hat{f}( x_{0})]^{2}\\
 & =\E[ f( x_{0}) -\E[\hat{f}( x_{0})]]^{2} +2\E[ f( x_{0}) -\E[\hat{f}( x_{0})] \ ][ \E[\hat{f}( x_{0})] -\hat{f}( x_{0})] +\E[ \E[\hat{f}( x_{0})] -\hat{f}( x_{0})]^{2}\\
 & =( f( x_{0}) -\E[\hat{f}( x_{0})])^{2} \  + \Var[ f( x_{0})].
\end{align}
$$
Secondly note that $y_0$ and the training data are independent. In addition $\mathrm{E}[ \epsilon ] \ =\ 0 $ Then expanding equation $\ref{eq:1.1}$ and using equation $\ref{eq:1.2}$ the decomposition becomes 

$$
\begin{align}
\E[ y_{0} -\hat{f}( x_{0})]^{2} & \ =\E[ f( x_{0}) +\epsilon -\hat{f}( x_{0})]^{2}\\
 & =\E[ \epsilon - \E[ \epsilon ]]^{2} +2\E[ \epsilon ] \E[ f( x_{0}) -\hat{f}( x_{0})] +\E[ f( x_{0}) -\hat{f}( x_{0})]^{2} \ \\
 & =\Var[ \epsilon ] + [ f( x_{0}) - \E[\hat{f}( x_{0})]]^{2}   + \Var[ f( x_{0})].
\end{align}
$$
## c)
Since variance and sqared expression cannot be negative each of the three terms constributes to the expected test MSE at $x_0$. The

$\pmb{\Var[ \epsilon ]}$ is the **irreducible errror**. It is independenet of the model and cannot be reduced. 


$\pmb{\Var[ f( x_{0})]}$ is the **variance** of the prediction at $x_{0}. It is a measure of how much the model will change based on different training data. In genereal the variance of the model increases with the flexibileity of the model. 

$[ f( x_{0}) - \E[\hat{f}( x_{0})]]^{2}$ is the **sqared bias** at $x_{0}$, also denoted $\mathrm{Bias}(\hat{f}( x_{0}))]^{2}$  In general less flexible models will have high bias. 


In genereal the a flexible models will have higer variance and lower bias. A good model will have low bias and low variance. 
## d)

## e)

## f)

## g)

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
#id <- "1nLen1ckdnX4P9n8ShZeU7zbXpLc7qiwt" # google file ID
#d.worm <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
#head(d.worm)
```




## a)


## b) 
Below you have to complete the code and then replace `eval=FALSE` by `eval=TRUE` in the chunk options:
```{r, eval=FALSE, echo=TRUE}
ggplot(d.worm,aes(x= ... ,y=  ... ,colour= ...)) + geom_point() + theme_bw()
```

Note that the default figure width and height have been set globally as `fig.width=4, fig.height=3`, but if you would like to change that (e.g., due to space constraints), you can include a different width and height directly into the chunk options, again using `fig.width=..., fig.height=...`.

## c)

## d)

## e)

## f)

## g)


# Problem 3

## a)

## b)

## c)

## d)

## e)

## f)

## g)

## h)


# Problem 4

## a)

## b)

## c)

## d)



