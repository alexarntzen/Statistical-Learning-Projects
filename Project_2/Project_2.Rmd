---
title: 'Compulsory exercise 2: Group 12'
author: "Emma Skarnes, Håkon Noren  and Alexander Johan Arntzen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4268 Statistical Learning V2020
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(eval = TRUE, echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)

```

```{r,eval=TRUE,echo=FALSE}
#install.packages("knitr") #probably already installed
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ISLR")
#install.packages("MASS")
#install.packages("GGally")
#install.packages("glmnet")
#install.packages("e1071")
#install.packages("tree")
#install.packages("leaps")
#install.packages("randomForest")
#install.packages("gbm")
#install.packages("ggfortify")

library(knitr)
library(rmarkdown)
library(GGally)
library(ggplot2)
library(ggfortify)
library(MASS)
library(ISLR)
library(dplyr)
library(boot)
library(Rfast)
library(formatR)
library(e1071)
library(corrplot)
library(tree)
theme_set(theme_bw())
```

# Problem 1

## a)

## b)

## c)

## d)

## e)



# Problem 2

## a)

## b)

## c)

## d)



# Problem 3

## a)

## b)

## c)



# Problem 4
In this problem we use the data set of diabetes from a population of women of Pima Indian heritage in the US. We split the data set into a training set of 300 observations, where 200 are non-diabetic and 100 are diabetic, and a test with of 232 observations, where 155 are non-diabetic and 77 are diabetic. 

``` {r, eval = TRUE, echo = FALSE}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download",
                           id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
```

## a)
```{r, eval = TRUE, echo = TRUE}
# DENNE FJERNES FØR INNLEVERING, LAR DEN STÅ ENN SÅ LENGE SÅ DEN SOM SKAL SE OVER HAR NOE Å TA UTGANGSPUNKT I
summary(d.train)
cor(d.train)
corrplot(cor(d.train))
ggpairs(d.train)
```
TRUE, TRUE, TRUE, TRUE (???)(Skummelt med så mange like, ass)

## b)
To fit a support vector classifier and a support vector machine to the problem, the response variable $\texttt{diabetes}$ must first be converted into a factor variable.
```{r, eval = TRUE, echo = FALSE} 
d.train$diabetes = as.factor(d.train$diabetes)
d.test$diabetes = as.factor(d.test$diabetes)
```
We start by fitting a support vector classifier, which has a linear boundary. To find a good cost parameter, cross-validation is used. The confusion table and the misclassification error reported are for the test set.

```{r, eval = TRUE, echo = TRUE}
svc = svm(diabetes ~ ., data = d.train, kernel = "linear", cost = 0.1, scale = FALSE)

# Find best cost for SVC
set.seed(1)
tune.cost = tune(method = "svm", diabetes ~ ., data = d.train, kernel = "linear", 
                ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.cost)                 # 0.1 is the best cost
svc.bestmod = tune.cost$best.model

svc.pred = predict(svc.bestmod, d.test)
svc.ct = table(predict = svc.pred, truth = d.test$diabetes)  # Confusion table
svc.mcr = 1 - sum(diag(svc.ct))/sum(svc.ct)                  # Misclassification error rate
print(paste0("Confusion table: ", svc.ct))
print(paste0("Misclassification error rate: ", svc.mcr))
```

Similarly, we now fit a support vector machine with a radial boundary. Cross-validation is now used to find the optimal combination of cost and $\gamma$ parameters. 
```{r, eval = TRUE, echo = TRUE}
svmfit = svm(diabetes ~., data = d.train, kernel = "radial", gamma = 0.5, cost = 1, scale = FALSE)

# Find the best cost and gamma for SVM
set.seed(2)
tune.costgamma = tune(method = "svm", diabetes ~ ., data = d.train, kernel = "radial", 
                ranges = list(cost = c(0.1, 1, 5, 10, 100), 
                              gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.costgamma)
svm.bestmod = tune.costgamma$best.model

svm.pred = predict(svm.bestmod, d.test)
svm.ct = table(predict = svm.pred, truth = d.test$diabetes)
svm.mcr = 1 - sum(diag(svm.ct))/sum(svm.ct)
print(paste0("Confusion table: ", svm.ct))
print(paste0("Misclassification error rate: ", svm.mcr))
```
Based on the confusion tables and their associated misclassification error rates, we can see that the support vector classifier performs better than the support vector machine, with a misclassification error rate of $0.228$ instead of $0.259$ for the support vector machine. Out of these two classifiers, we thus prefer the support vector classifier, even if the difference is relatively small.  
The SVC also has both a higher sensitivity and specificity. The sensitivities for the SVC and SVM, respectively, are $0.884$ and $0.858$. The respective specificities are $0.545$ and $0.507$.  

## c)
We now compare the performance of the two classifiers from 4b) to a classification tree. As for the SVC and SVM we fit a model, now a classification tree, to our training set, before we use the test set to find the confusion table and misclassification error rate of the method.
```{r, eval = TRUE, echo = TRUE}
d.tree = tree(diabetes ~ ., data = d.train)
tree.pred = predict(d.tree, d.test$diabetes, type = "class")
tree.ct = table(tree.pred, d.test$diabetes)
tree.mcr = 1 - sum(diag(tree.ct))/sum(tree.ct)
print(paste0("Confusion table: ", tree.ct))
print(paste0("Misclassification error rate: ", tree.mcr))
```
Note that $\texttt{cv.tree}$ automatically does 10-fold cross-validation, so we don't have to prune the tree in the same way that we had to find the optimal cost and $\gamma$ parameters in Problem 4b). We observe from the confusion table and the misclassification error rate that the classification tree performed better than the support vector machine, but worse than the support vector classifier. 
The sensitivity of this method is $0.813$, which is lower than the values for both of the classifiers from 4b). However, the specificity, $0.636$, is better. 
BØR VI LAGE ET PLOT MED FALSE POSITIVE RATE VS. TRUE POSITIVE RATE HER FOR Å SAMMENLIGNE?

Classification trees are in general, and especially for non-staticians, much easier to interpret than other classification methods. The structure and visualization of the tree is what makes it so easy to interpret, and the method is thus more used by non-staticians than SVM. In addition, the structure of the classification tree shows which predictor is the most important, by splitting on this predictor first. 
Neither classification trees nor SVMs make a huge amount of assumptions, for example about the distribution of the data. Thus they are less affected by outliers, and in that matter no method is preferred over the other one. Trees are often computed quite fast, but the greedy algorithm might not be as accurate as the SVM. However, the SVM can be harder to train, and without good parameters a good performance is not guaranteed - while trees are often very good classifiers.
Finally, the process where the SVM projects the feature space into a kernel space before it is projected back to the original feature space, can produce a non-linear decision boundary that performs better than the hyperrectangles of the classification trees.
GJERNE LEGG TIL MER OM DERE HAR GODE PUNKTER!!!


## d)
(i) FALSE
(ii) FALSE
(iii) TRUE ???
(iv) TRUE ???

(Husk å skrive om til riktig form)

## e)
After manipulating the logistic function a little bit, we obtain
$$
\begin{split}
  log \frac{P(x_i)}{1-P(x_i)} = f(x_i) \\
  \implies P(y_i | x; \beta) = \frac{e^{f(x_i)}}{1 + e^{f(x_i)} = \frac{1}{1 + e^{-f(x_i)}}.
\end{split}
$$
Let $\sigma$ be a function such that $P(y_i = 1 | x_i) = \sigma(x_i)$ and $P(y_i = -1 | x_i) = 1 - \sigma(x_i) = \sigma(-x_i)$, where the last equality comes from the properties of the function and can easily be seen by rewriting the equation a little bit. The cumulative distribution function can then be written as $P(y_i | x_i) = \sigma(x_i)^{y_i} (1-\sigma(x_i))^{1-y_i}$, which we recognize as a binomial PMF with $p = \sigma(x_i)$.

Furthermore, the log-likelihood function for this logistic regression function is given by
$$
\begin{split}
l(z) &= -log(\Pi_{i=1}^n P(y_i | x_i)) = - \sum_{i=1}^n log(P(y_i | x_i)) = - \sum_{i=1}^n log \bigg{(}\sigma(x_i)^{y_i} (1-\sigma(x_i))^{1-y_i}\bigg{)} \\
&= - \sum_{i=1}^n \bigg{(}y_i log (\sigma(x_i)) + (1-y_i) log (1-\sigma(x_i))\bigg{)} = -\sum_{i=1}^n \bigg{(}y_i(log(\sigma(x_i)) - log(-\sigma(x_i))) + log(-\sigma(x_i))\bigg{)}\\
&= -\sum_{i=1}^n \bigg{(} y_i log(\frac{\sigma(x_i)}{1-\sigma(x_i)}) + log(\sigma(-x_i)) \bigg{)} = -\sum_{i=1}^n (y_i x_i + log(-\sigma(x_i)) )  
 \\


&=  \sum_{i=1}^n log(1 + e^{-y_i x_i})
\end{split}
$$
Then, since $f(x_i)$ corresponds to the linear predictor in logistic regression, we can replace $x_i$ by $f(x_i)$ in the result above, which shows that the deviance for the $y = \pm 1$ encoding in logistic regression is the same as the given loss function $log(1 + e^{y_i f(x_i)})$.


# Problem 5

## a)

## b)

## c)

## d)

## e)

## f)